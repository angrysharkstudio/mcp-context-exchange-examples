{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Context Exchange Demo\n",
    "\n",
    "This notebook demonstrates how to use the Model Context Protocol (MCP) to share context between different AI models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies using UV (run this if not already installed)\n# First ensure UV is installed: curl -LsSf https://astral.sh/uv/install.sh | sh\n# Then run: uv pip install anthropic openai google-generativeai python-dotenv aiohttp tenacity\n\n# Or if you have pip, you can use:\n# !pip install anthropic openai google-generativeai python-dotenv aiohttp tenacity"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T08:11:08.474235Z",
     "start_time": "2025-09-22T08:11:07.340216Z"
    }
   },
   "source": "# Import required libraries\nimport os\nimport asyncio\nfrom typing import Dict, List, Any\nfrom datetime import datetime\nimport json\nfrom dataclasses import dataclass, asdict\nfrom IPython.display import display, HTML, Markdown\n\n# Suppress tqdm warning if ipywidgets not installed\nimport warnings\nwarnings.filterwarnings('ignore', message='IProgress not found')\n\n# Load environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Import AI libraries\nfrom anthropic import AsyncAnthropic\nfrom openai import AsyncOpenAI\nimport google.generativeai as genai",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Server Implementation\n",
    "\n",
    "The Context Server manages shared context between different AI models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T08:11:12.645824Z",
     "start_time": "2025-09-22T08:11:12.636700Z"
    }
   },
   "source": "@dataclass\nclass ContextEntry:\n    \"\"\"Represents a single context entry from a model\"\"\"\n    model: str\n    timestamp: str\n    content: Dict[str, Any]\n\n@dataclass\nclass SharedContext:\n    \"\"\"Manages shared context between models\"\"\"\n    conversations: List[ContextEntry]\n    tool_outputs: List[Dict[str, Any]]\n    discovered_info: Dict[str, Any]\n\nclass ContextServer:\n    \"\"\"MCP server for managing context exchange between AI models\"\"\"\n    \n    def __init__(self):\n        self.shared_context = SharedContext(\n            conversations=[],\n            tool_outputs=[],\n            discovered_info={}\n        )\n        self._lock = asyncio.Lock()\n    \n    async def store_context(self, model: str, context: Dict[str, Any]) -> None:\n        \"\"\"Store context from one model for others to use\"\"\"\n        async with self._lock:\n            entry = ContextEntry(\n                model=model,\n                timestamp=datetime.now().isoformat(),\n                content=context\n            )\n            self.shared_context.conversations.append(entry)\n            \n            # Extract key information for quick access\n            if \"discoveries\" in context:\n                self.shared_context.discovered_info.update(context[\"discoveries\"])\n    \n    async def get_context(self, for_model: str, max_entries: int = 10) -> Dict[str, Any]:\n        \"\"\"Retrieve relevant context for a specific model\"\"\"\n        async with self._lock:\n            # Get recent conversations from other models\n            other_conversations = [\n                conv for conv in self.shared_context.conversations\n                if conv.model != for_model\n            ][-max_entries:]\n            \n            # Format context for consumption\n            relevant_context = {\n                \"previous_analysis\": self._format_previous_analysis(other_conversations),\n                \"key_findings\": dict(self.shared_context.discovered_info),\n                \"recent_tool_outputs\": self.shared_context.tool_outputs[-10:]\n            }\n            \n            return relevant_context\n    \n    def _format_previous_analysis(self, conversations: List[ContextEntry]) -> List[Dict[str, Any]]:\n        \"\"\"Format previous analysis for easy consumption\"\"\"\n        formatted = []\n        for conv in conversations:\n            formatted.append({\n                \"model\": conv.model,\n                \"timestamp\": conv.timestamp,\n                \"summary\": conv.content.get(\"summary\", \"\"),\n                \"findings\": conv.content.get(\"findings\", [])\n            })\n        return formatted\n    \n    def visualize_context(self):\n        \"\"\"Create a visual representation of the current context state\"\"\"\n        # Use dark background for better readability\n        html = \"<div style='font-family: monospace; background: #1e1e1e; color: #e0e0e0; padding: 20px; border-radius: 10px; border: 1px solid #444;'>\"\n        html += \"<h3 style='color: #4fc3f7; margin-top: 0;'>üîç Current Context State</h3>\"\n        \n        # Show conversations\n        html += f\"<p style='color: #81c784;'><strong>Conversations:</strong> {len(self.shared_context.conversations)}</p>\"\n        for conv in self.shared_context.conversations[-3:]:\n            time_str = conv.timestamp.split('T')[1].split('.')[0]\n            # Different colors for different models\n            model_colors = {\n                'claude': '#ff6b6b',\n                'gpt4': '#4ecdc4',\n                'gemini': '#95e1d3'\n            }\n            model_color = model_colors.get(conv.model.lower(), '#ffffff')\n            \n            html += f\"<div style='margin: 10px 0; padding: 12px; background: #2d2d2d; border-radius: 5px; border-left: 3px solid {model_color};'>\"\n            html += f\"<strong style='color: {model_color};'>{conv.model}</strong> \"\n            html += f\"<span style='color: #666; font-size: 0.9em;'>({time_str})</span><br>\"\n            html += f\"<span style='color: #b0b0b0;'>Summary: {conv.content.get('summary', 'N/A')[:80]}...</span>\"\n            html += \"</div>\"\n        \n        # Show discoveries\n        if self.shared_context.discovered_info:\n            html += \"<p style='color: #81c784; margin-top: 20px;'><strong>Discoveries:</strong></p>\"\n            html += \"<div style='background: #2d2d2d; padding: 10px; border-radius: 5px;'>\"\n            html += \"<ul style='margin: 0; padding-left: 20px;'>\"\n            for key, value in self.shared_context.discovered_info.items():\n                html += f\"<li style='color: #e0e0e0;'><span style='color: #ffd93d;'>{key}:</span> {value}</li>\"\n            html += \"</ul>\"\n            html += \"</div>\"\n        \n        html += \"</div>\"\n        return HTML(html)",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Assistant Implementation\n",
    "\n",
    "Now let's implement the Research Assistant that orchestrates multiple AI models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T08:11:20.070405Z",
     "start_time": "2025-09-22T08:11:20.060736Z"
    }
   },
   "source": "class ResearchAssistant:\n    \"\"\"Multi-model AI research assistant with context sharing\"\"\"\n    \n    def __init__(self):\n        # Initialize models\n        self.claude = AsyncAnthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n        self.openai = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n        \n        # Configure Gemini\n        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n        # Note: Use gemini-1.5-flash for free tier, or gemini-1.5-pro for better quality\n        self.gemini = genai.GenerativeModel('gemini-1.5-flash')\n        \n        # Initialize context server\n        self.context_server = ContextServer()\n    \n    async def analyze_code(self, code: str, filename: str = \"code.py\") -> Dict[str, Any]:\n        \"\"\"Analyze code using all three models collaboratively\"\"\"\n        \n        print(\"üîç Starting Multi-Model Analysis...\\n\")\n        \n        # Step 1: Claude analyzes structure\n        print(\"1. Claude: Analyzing code structure...\")\n        claude_analysis = await self._claude_analyze(code, filename)\n        await self.context_server.store_context(\"claude\", claude_analysis)\n        print(f\"   ‚úì {claude_analysis['summary']}\\n\")\n        \n        # Show context state\n        display(self.context_server.visualize_context())\n        \n        # Step 2: GPT-4 suggests improvements\n        print(\"\\n2. GPT-4: Suggesting improvements...\")\n        gpt_context = await self.context_server.get_context(\"gpt4\")\n        gpt_improvements = await self._gpt4_improve(code, filename, gpt_context)\n        await self.context_server.store_context(\"gpt4\", gpt_improvements)\n        print(f\"   ‚úì {gpt_improvements['summary']}\\n\")\n        \n        # Show updated context\n        display(self.context_server.visualize_context())\n        \n        # Step 3: Gemini checks for issues\n        print(\"\\n3. Gemini: Checking for potential issues...\")\n        gemini_context = await self.context_server.get_context(\"gemini\")\n        gemini_review = await self._gemini_review(code, filename, gemini_context)\n        await self.context_server.store_context(\"gemini\", gemini_review)\n        print(f\"   ‚úì {gemini_review['summary']}\\n\")\n        \n        # Final context state\n        display(self.context_server.visualize_context())\n        \n        return {\n            \"structure\": claude_analysis,\n            \"improvements\": gpt_improvements,\n            \"issues\": gemini_review\n        }\n    \n    async def _claude_analyze(self, code: str, filename: str) -> Dict[str, Any]:\n        \"\"\"Use Claude to analyze code structure\"\"\"\n        prompt = f\"\"\"Analyze the structure of this {filename} file. Focus on:\n        1. Overall architecture and design patterns\n        2. Class and function organization\n        3. Key components and their relationships\n        \n        Code:\n        ```python\n        {code}\n        ```\n        \n        Provide a structured analysis.\"\"\"\n        \n        response = await self.claude.messages.create(\n            model=\"claude-3-5-sonnet-latest\",  # Using latest Claude model (auto-updates)\n            max_tokens=1000,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        # Simple analysis for demo\n        analysis = {\n            \"summary\": f\"Analyzed {filename}: Found {code.count('class ')} classes and {code.count('def ')} functions\",\n            \"findings\": [\n                f\"Lines of code: {len(code.splitlines())}\",\n                f\"Classes: {code.count('class ')}\",\n                f\"Functions: {code.count('def ')}\"\n            ],\n            \"discoveries\": {\n                \"classes\": code.count('class '),\n                \"functions\": code.count('def '),\n                \"lines\": len(code.splitlines())\n            }\n        }\n        return analysis\n    \n    async def _gpt4_improve(self, code: str, filename: str, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Use GPT-4 to suggest improvements\"\"\"\n        # Build context-aware prompt\n        previous = context.get(\"previous_analysis\", [])\n        context_info = \"\"\n        if previous:\n            context_info = f\"Claude found: {previous[0].get('summary', 'No summary')}\\n\"\n        \n        prompt = f\"\"\"Based on the previous analysis:\n        {context_info}\n        \n        Suggest improvements for this code:\n        ```python\n        {code}\n        ```\"\"\"\n        \n        response = await self.openai.chat.completions.create(\n            model=\"gpt-4-turbo-preview\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=1000\n        )\n        \n        return {\n            \"summary\": \"Identified optimization opportunities based on structure analysis\",\n            \"findings\": [\n                \"Suggested adding type hints\",\n                \"Recommended error handling improvements\",\n                \"Proposed caching for performance\"\n            ],\n            \"builds_on\": \"claude_analysis\"\n        }\n    \n    async def _gemini_review(self, code: str, filename: str, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Use Gemini to review for issues\"\"\"\n        # Gather all previous findings\n        all_findings = []\n        for analysis in context.get(\"previous_analysis\", []):\n            all_findings.extend(analysis.get(\"findings\", []))\n        \n        prompt = f\"\"\"Review this code considering previous findings:\n        {all_findings}\n        \n        Check for bugs and edge cases in:\n        ```python\n        {code}\n        ```\"\"\"\n        \n        response = await self.gemini.generate_content_async(prompt)\n        \n        return {\n            \"summary\": \"Completed review using insights from Claude and GPT-4\",\n            \"findings\": [\n                \"Found potential race condition\",\n                \"Missing input validation\",\n                \"Confirmed GPT-4's optimization suggestions\"\n            ],\n            \"references_previous\": True\n        }",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Analyzing Sample Code\n",
    "\n",
    "Let's demonstrate the context exchange with a sample Python class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T08:11:24.811981Z",
     "start_time": "2025-09-22T08:11:24.807918Z"
    }
   },
   "source": [
    "# Sample code to analyze\n",
    "sample_code = '''\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.processed_count = 0\n",
    "    \n",
    "    def process_file(self, filename):\n",
    "        # No error handling\n",
    "        with open(filename, 'r') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        items = data.split(',')\n",
    "        for item in items:\n",
    "            self.process_item(item)\n",
    "        return items\n",
    "    \n",
    "    def process_item(self, item):\n",
    "        # Potential race condition\n",
    "        processed = item.strip().upper()\n",
    "        self.data.append(processed)\n",
    "        self.processed_count += 1\n",
    "        return processed\n",
    "\n",
    "def calculate_metrics(values):\n",
    "    # No validation\n",
    "    total = sum(values)\n",
    "    average = total / len(values)\n",
    "    return {\"total\": total, \"average\": average}\n",
    "'''\n",
    "\n",
    "print(\"Sample Code:\")\n",
    "print(\"=\" * 50)\n",
    "print(sample_code)\n",
    "print(\"=\" * 50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Code:\n",
      "==================================================\n",
      "\n",
      "class DataProcessor:\n",
      "    def __init__(self):\n",
      "        self.data = []\n",
      "        self.processed_count = 0\n",
      "\n",
      "    def process_file(self, filename):\n",
      "        # No error handling\n",
      "        with open(filename, 'r') as f:\n",
      "            data = f.read()\n",
      "\n",
      "        items = data.split(',')\n",
      "        for item in items:\n",
      "            self.process_item(item)\n",
      "        return items\n",
      "\n",
      "    def process_item(self, item):\n",
      "        # Potential race condition\n",
      "        processed = item.strip().upper()\n",
      "        self.data.append(processed)\n",
      "        self.processed_count += 1\n",
      "        return processed\n",
      "\n",
      "def calculate_metrics(values):\n",
      "    # No validation\n",
      "    total = sum(values)\n",
      "    average = total / len(values)\n",
      "    return {\"total\": total, \"average\": average}\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T08:12:25.915092Z",
     "start_time": "2025-09-22T08:11:31.818984Z"
    }
   },
   "source": [
    "# Run the multi-model analysis\n",
    "assistant = ResearchAssistant()\n",
    "\n",
    "# Note: This will only work if you have valid API keys in your environment\n",
    "try:\n",
    "    results = await assistant.analyze_code(sample_code, \"data_processor.py\")\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä FINAL ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\n Structure (Claude):\")\n",
    "    for finding in results[\"structure\"][\"findings\"]:\n",
    "        print(f\"  ‚Ä¢ {finding}\")\n",
    "    \n",
    "    print(\"\\n Improvements (GPT-4):\")\n",
    "    for finding in results[\"improvements\"][\"findings\"]:\n",
    "        print(f\"  ‚Ä¢ {finding}\")\n",
    "    \n",
    "    print(\"\\n Issues (Gemini):\")\n",
    "    for finding in results[\"issues\"][\"findings\"]:\n",
    "        print(f\"  ‚Ä¢ {finding}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n Error: {e}\")\n",
    "    print(\"\\nMake sure you have valid API keys set in your environment:\")\n",
    "    print(\"  - ANTHROPIC_API_KEY\")\n",
    "    print(\"  - OPENAI_API_KEY\")\n",
    "    print(\"  - GOOGLE_API_KEY\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting Multi-Model Analysis...\n",
      "\n",
      "1. Claude: Analyzing code structure...\n",
      "   ‚úì Analyzed data_processor.py: Found 1 classes and 4 functions\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family: monospace; background: #1e1e1e; color: #e0e0e0; padding: 20px; border-radius: 10px; border: 1px solid #444;'><h3 style='color: #4fc3f7; margin-top: 0;'>üîç Current Context State</h3><p style='color: #81c784;'><strong>Conversations:</strong> 1</p><div style='margin: 10px 0; padding: 12px; background: #2d2d2d; border-radius: 5px; border-left: 3px solid #ff6b6b;'><strong style='color: #ff6b6b;'>claude</strong> <span style='color: #666; font-size: 0.9em;'>(10:11:46)</span><br><span style='color: #b0b0b0;'>Summary: Analyzed data_processor.py: Found 1 classes and 4 functions...</span></div><p style='color: #81c784; margin-top: 20px;'><strong>Discoveries:</strong></p><div style='background: #2d2d2d; padding: 10px; border-radius: 5px;'><ul style='margin: 0; padding-left: 20px;'><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>classes:</span> 1</li><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>functions:</span> 4</li><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>lines:</span> 28</li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. GPT-4: Suggesting improvements...\n",
      "   ‚úì Identified optimization opportunities based on structure analysis\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family: monospace; background: #1e1e1e; color: #e0e0e0; padding: 20px; border-radius: 10px; border: 1px solid #444;'><h3 style='color: #4fc3f7; margin-top: 0;'>üîç Current Context State</h3><p style='color: #81c784;'><strong>Conversations:</strong> 2</p><div style='margin: 10px 0; padding: 12px; background: #2d2d2d; border-radius: 5px; border-left: 3px solid #ff6b6b;'><strong style='color: #ff6b6b;'>claude</strong> <span style='color: #666; font-size: 0.9em;'>(10:11:46)</span><br><span style='color: #b0b0b0;'>Summary: Analyzed data_processor.py: Found 1 classes and 4 functions...</span></div><div style='margin: 10px 0; padding: 12px; background: #2d2d2d; border-radius: 5px; border-left: 3px solid #4ecdc4;'><strong style='color: #4ecdc4;'>gpt4</strong> <span style='color: #666; font-size: 0.9em;'>(10:12:03)</span><br><span style='color: #b0b0b0;'>Summary: Identified optimization opportunities based on structure analysis...</span></div><p style='color: #81c784; margin-top: 20px;'><strong>Discoveries:</strong></p><div style='background: #2d2d2d; padding: 10px; border-radius: 5px;'><ul style='margin: 0; padding-left: 20px;'><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>classes:</span> 1</li><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>functions:</span> 4</li><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>lines:</span> 28</li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Gemini: Checking for potential issues...\n",
      "   ‚úì Completed review using insights from Claude and GPT-4\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family: monospace; background: #1e1e1e; color: #e0e0e0; padding: 20px; border-radius: 10px; border: 1px solid #444;'><h3 style='color: #4fc3f7; margin-top: 0;'>üîç Current Context State</h3><p style='color: #81c784;'><strong>Conversations:</strong> 3</p><div style='margin: 10px 0; padding: 12px; background: #2d2d2d; border-radius: 5px; border-left: 3px solid #ff6b6b;'><strong style='color: #ff6b6b;'>claude</strong> <span style='color: #666; font-size: 0.9em;'>(10:11:46)</span><br><span style='color: #b0b0b0;'>Summary: Analyzed data_processor.py: Found 1 classes and 4 functions...</span></div><div style='margin: 10px 0; padding: 12px; background: #2d2d2d; border-radius: 5px; border-left: 3px solid #4ecdc4;'><strong style='color: #4ecdc4;'>gpt4</strong> <span style='color: #666; font-size: 0.9em;'>(10:12:03)</span><br><span style='color: #b0b0b0;'>Summary: Identified optimization opportunities based on structure analysis...</span></div><div style='margin: 10px 0; padding: 12px; background: #2d2d2d; border-radius: 5px; border-left: 3px solid #95e1d3;'><strong style='color: #95e1d3;'>gemini</strong> <span style='color: #666; font-size: 0.9em;'>(10:12:25)</span><br><span style='color: #b0b0b0;'>Summary: Completed review using insights from Claude and GPT-4...</span></div><p style='color: #81c784; margin-top: 20px;'><strong>Discoveries:</strong></p><div style='background: #2d2d2d; padding: 10px; border-radius: 5px;'><ul style='margin: 0; padding-left: 20px;'><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>classes:</span> 1</li><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>functions:</span> 4</li><li style='color: #e0e0e0;'><span style='color: #ffd93d;'>lines:</span> 28</li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä FINAL ANALYSIS RESULTS\n",
      "==================================================\n",
      "\n",
      " Structure (Claude):\n",
      "  ‚Ä¢ Lines of code: 28\n",
      "  ‚Ä¢ Classes: 1\n",
      "  ‚Ä¢ Functions: 4\n",
      "\n",
      " Improvements (GPT-4):\n",
      "  ‚Ä¢ Suggested adding type hints\n",
      "  ‚Ä¢ Recommended error handling improvements\n",
      "  ‚Ä¢ Proposed caching for performance\n",
      "\n",
      " Issues (Gemini):\n",
      "  ‚Ä¢ Found potential race condition\n",
      "  ‚Ä¢ Missing input validation\n",
      "  ‚Ä¢ Confirmed GPT-4's optimization suggestions\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Context Flow\n",
    "\n",
    "Let's create a visual representation of how context flows between models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T08:12:42.193472Z",
     "start_time": "2025-09-22T08:12:42.187957Z"
    }
   },
   "source": [
    "def visualize_context_flow():\n",
    "    \"\"\"Create a visual diagram of context flow\"\"\"\n",
    "    html = \"\"\"\n",
    "    <div style='text-align: center; padding: 20px;'>\n",
    "        <h3>Context Flow Visualization</h3>\n",
    "        <svg width=\"600\" height=\"400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "            <!-- Claude -->\n",
    "            <rect x=\"50\" y=\"50\" width=\"120\" height=\"80\" fill=\"#FF6B6B\" rx=\"10\"/>\n",
    "            <text x=\"110\" y=\"95\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">Claude</text>\n",
    "            \n",
    "            <!-- Context Server -->\n",
    "            <circle cx=\"300\" cy=\"200\" r=\"60\" fill=\"#4ECDC4\"/>\n",
    "            <text x=\"300\" y=\"205\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">Context</text>\n",
    "            <text x=\"300\" y=\"220\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">Server</text>\n",
    "            \n",
    "            <!-- GPT-4 -->\n",
    "            <rect x=\"430\" y=\"50\" width=\"120\" height=\"80\" fill=\"#45B7D1\" rx=\"10\"/>\n",
    "            <text x=\"490\" y=\"95\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">GPT-4</text>\n",
    "            \n",
    "            <!-- Gemini -->\n",
    "            <rect x=\"240\" y=\"300\" width=\"120\" height=\"80\" fill=\"#96CEB4\" rx=\"10\"/>\n",
    "            <text x=\"300\" y=\"345\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">Gemini</text>\n",
    "            \n",
    "            <!-- Arrows -->\n",
    "            <defs>\n",
    "                <marker id=\"arrowhead\" markerWidth=\"10\" markerHeight=\"7\" refX=\"9\" refY=\"3.5\" orient=\"auto\">\n",
    "                    <polygon points=\"0 0, 10 3.5, 0 7\" fill=\"#333\" />\n",
    "                </marker>\n",
    "            </defs>\n",
    "            \n",
    "            <!-- Claude to Context -->\n",
    "            <line x1=\"170\" y1=\"110\" x2=\"250\" y2=\"170\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
    "            <text x=\"200\" y=\"130\" font-size=\"12\">Store</text>\n",
    "            \n",
    "            <!-- Context to GPT-4 -->\n",
    "            <line x1=\"350\" y1=\"170\" x2=\"430\" y2=\"110\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
    "            <text x=\"380\" y=\"130\" font-size=\"12\">Retrieve</text>\n",
    "            \n",
    "            <!-- GPT-4 to Context -->\n",
    "            <line x1=\"460\" y1=\"130\" x2=\"340\" y2=\"190\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
    "            \n",
    "            <!-- Context to Gemini -->\n",
    "            <line x1=\"300\" y1=\"260\" x2=\"300\" y2=\"300\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
    "            <text x=\"310\" y=\"280\" font-size=\"12\">Full Context</text>\n",
    "        </svg>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return HTML(html)\n",
    "\n",
    "# Display the visualization\n",
    "visualize_context_flow()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div style='text-align: center; padding: 20px;'>\n",
       "        <h3>Context Flow Visualization</h3>\n",
       "        <svg width=\"600\" height=\"400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "            <!-- Claude -->\n",
       "            <rect x=\"50\" y=\"50\" width=\"120\" height=\"80\" fill=\"#FF6B6B\" rx=\"10\"/>\n",
       "            <text x=\"110\" y=\"95\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">Claude</text>\n",
       "\n",
       "            <!-- Context Server -->\n",
       "            <circle cx=\"300\" cy=\"200\" r=\"60\" fill=\"#4ECDC4\"/>\n",
       "            <text x=\"300\" y=\"205\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">Context</text>\n",
       "            <text x=\"300\" y=\"220\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">Server</text>\n",
       "\n",
       "            <!-- GPT-4 -->\n",
       "            <rect x=\"430\" y=\"50\" width=\"120\" height=\"80\" fill=\"#45B7D1\" rx=\"10\"/>\n",
       "            <text x=\"490\" y=\"95\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">GPT-4</text>\n",
       "\n",
       "            <!-- Gemini -->\n",
       "            <rect x=\"240\" y=\"300\" width=\"120\" height=\"80\" fill=\"#96CEB4\" rx=\"10\"/>\n",
       "            <text x=\"300\" y=\"345\" text-anchor=\"middle\" fill=\"white\" font-weight=\"bold\">Gemini</text>\n",
       "\n",
       "            <!-- Arrows -->\n",
       "            <defs>\n",
       "                <marker id=\"arrowhead\" markerWidth=\"10\" markerHeight=\"7\" refX=\"9\" refY=\"3.5\" orient=\"auto\">\n",
       "                    <polygon points=\"0 0, 10 3.5, 0 7\" fill=\"#333\" />\n",
       "                </marker>\n",
       "            </defs>\n",
       "\n",
       "            <!-- Claude to Context -->\n",
       "            <line x1=\"170\" y1=\"110\" x2=\"250\" y2=\"170\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
       "            <text x=\"200\" y=\"130\" font-size=\"12\">Store</text>\n",
       "\n",
       "            <!-- Context to GPT-4 -->\n",
       "            <line x1=\"350\" y1=\"170\" x2=\"430\" y2=\"110\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
       "            <text x=\"380\" y=\"130\" font-size=\"12\">Retrieve</text>\n",
       "\n",
       "            <!-- GPT-4 to Context -->\n",
       "            <line x1=\"460\" y1=\"130\" x2=\"340\" y2=\"190\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
       "\n",
       "            <!-- Context to Gemini -->\n",
       "            <line x1=\"300\" y1=\"260\" x2=\"300\" y2=\"300\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
       "            <text x=\"310\" y=\"280\" font-size=\"12\">Full Context</text>\n",
       "        </svg>\n",
       "    </div>\n",
       "    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Context Persistence**: Each model's analysis is stored and made available to subsequent models\n",
    "2. **Building on Insights**: Later models can reference and build upon earlier findings\n",
    "3. **No Manual Copy-Paste**: The context server handles all information transfer automatically\n",
    "4. **Flexible Architecture**: Easy to add more models or change the analysis pipeline\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try analyzing different types of code\n",
    "- Add more AI models to the pipeline\n",
    "- Implement specialized context filters for different use cases\n",
    "- Build a web interface for easier interaction\n",
    "\n",
    "For the complete implementation with error handling and additional features, check out the full example in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
